- hosts: 127.0.0.1
  connection: local
  vars:
    region: "{{ lookup('env', 'AWS_REGION') or 'eu-west-2' }}"
    regions:
      eu-west-2:
        ami_id: ami-02d37bf30f2944395 # The Who image ID from packer
        # The default VPC
        vpc_id: vpc-fa2e0792
    vpc_id: "{{ regions[region].vpc_id }}"
    ami_id: "{{ regions[region].ami_id }}"
    lc_num: 138

    elb_ssl_arn: "arn:aws:acm:eu-west-2:061126312678:certificate/3ac6d121-aed6-48a7-af80-6a021e05167c"
    old_lc_num: "{{ lc_num - 1 }}"
    aws_env: "{{ lookup('env', 'ENVIRONMENT') or 'test' }}"
  environment:
    AWS_REGION: "{{ region }}"
  tasks:
    - ec2_vpc_subnet_facts:
        filters:
          vpc-id: "{{ vpc_id }}"
      register: subnets

    - name: ELB security group
      ec2_group:
        name: "wcivf-elb-{{ aws_env }}"
        description: "ELB http security group"
        vpc_id: "{{ vpc_id }}"
        rules:
          - proto: tcp
            from_port: 80
            to_port: 80
            cidr_ip: 0.0.0.0/0
          - proto: tcp
            from_port: 443
            to_port: 443
            cidr_ip: 0.0.0.0/0
          - proto: tcp
            from_port: 22
            to_port: 22
            cidr_ip: 81.187.11.5/32
          - proto: tcp
            from_port: 22
            to_port: 22
            cidr_ip: 86.130.83.53/32
      register: sg_elb

    - name: Instance Security Group
      ec2_group:
        name: "wcivf-asg-{{ aws_env }}"
        description: "Allow access for SSH and HTTP from the ELB"
        vpc_id: "{{ vpc_id }}"
        rules:
          - proto: tcp
            from_port: 80
            to_port: 80
            group_id: "{{ sg_elb.group_id }}"
      register: sg_instance


    # this will fail first time becasue we're not adding any instances to it :(
    - name: Elastic Load Balancer
      ec2_elb_lb:
        name: "wcivf-{{ aws_env }}"
        state: present
        security_group_ids: "{{ sg_elb.group_id }}"
        # When the ELB is not having much traffic we'll only have ELB nodes in
        # 2 AZs, but instances in 3. This settings makes it send traffic to all
        # backend instances in that case
        cross_az_load_balancing: True
        connection_draining_timeout: 20
        listeners:
          - protocol: http
            load_balancer_port: 80
            instance_port: 80
          - protocol: https
            load_balancer_port: 443
            instance_port: 80
            instance_protocol: http
            ssl_certificate_id: "{{ elb_ssl_arn }}"
        health_check:
          ping_protocol: http
          ping_port: 80
          ping_path: "/_status_check/"
          response_timeout: 2
          interval: 30
          unhealthy_threshold: 5
          healthy_threshold: 2
        subnets: "{{ subnets.subnets | map(attribute='id') |list }}"
        tags:
          Env: "{{ aws_env }}"
          Name: "wcivf-{{ aws_env }}"
      register: elb_result



    - name: On-demand Launch Config
      ec2_lc:
        name: "wcivf_{{ aws_env }}-{{ lc_num }}"
        assign_public_ip: yes
        image_id: "{{ ami_id }}"
        instance_type: t3.large
        security_groups: ["{{ sg_instance.group_id }}", "sg-04c54174132078fa4"]
        user_data: "{{lookup('file', 'userdata.yml') }}"
        instance_profile_name: wcivf-packer-ami-builder
        volumes:
        - device_name: /dev/sda1
          volume_size: 35
          volume_type: gp2
          delete_on_termination: true
      register: launchconfig
      when: not spot_price is defined



    - name: On-demand Autoscailing group
      ec2_asg:
        name: "wcivf-asg-{{ aws_env }}"
        state: present
        tags:
          - Env: "{{ aws_env }}"
            Name: wcivf_asg
            CodeDeploy: wcivf
        desired_capacity: 1
        health_check_type: ELB
        launch_config_name: "{{launchconfig.name}}"
        load_balancers: ["{{ elb_result.elb.name }}"]
        max_size: 1
        min_size: 1
        replace_all_instances: "{{ replace_all|default(False) }}"
        termination_policies: [ OldestLaunchConfiguration, ClosestToNextInstanceHour ]
        # Yes, subnets go in vpc_zone_identifier. Blame Garethr.
        # vpc_zone_identifier: "{{ subnets.subnets | map(attribute='id') |list }}"
        # Wait for this long to replace old instances
        wait_timeout: 1200 # importing people can take ages
      when: not spot_price is defined


    - name: Delete old unused on-demand Launch Configs
      ec2_lc:
        name: "wcivf_{{ aws_env }}-{{ old_lc_num }}"
        state: absent

- import_playbook: tag_instances.yml
